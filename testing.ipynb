{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\GitHub\\LLM-Transformer\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/frankenstein.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text:str, tokenizer, max_length:int, stride:int) -> None:\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Encode the text\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx:int) ->tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")  # You can choose other encodings if needed\n",
    "\n",
    "# Example text (replace this with your own data)\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Hyperparameters\n",
    "max_length = 128\n",
    "stride = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = TextDataset(text, tokenizer, max_length, stride)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_length, embedding_dim=256, num_heads=8, num_layers=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                dropout=dropout,\n",
    "                activation='gelu'\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        positions = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        \n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, top_k=50):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = self.forward(input_ids)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                if top_k is not None:\n",
    "                    values, indices = torch.topk(logits, top_k)\n",
    "                    logits = torch.full_like(logits, -float('Inf')).scatter_(1, indices, values)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine vocabulary size\n",
    "vocab_size = tokenizer.n_vocab\n",
    "\n",
    "# Initialize the model\n",
    "model = MiniGPT(vocab_size=vocab_size, context_length=max_length)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] completed. Total Loss: 199.14232444763184\n",
      "Epoch [2/50] completed. Total Loss: 197.29787826538086\n",
      "Epoch [3/50] completed. Total Loss: 195.35980677604675\n",
      "Epoch [4/50] completed. Total Loss: 193.6152548789978\n",
      "Epoch [5/50] completed. Total Loss: 191.7218861579895\n",
      "Epoch [6/50] completed. Total Loss: 190.0475845336914\n",
      "Epoch [7/50] completed. Total Loss: 188.25259232521057\n",
      "Epoch [8/50] completed. Total Loss: 186.5202317237854\n",
      "Epoch [9/50] completed. Total Loss: 184.94557189941406\n",
      "Epoch [10/50] completed. Total Loss: 183.25842952728271\n",
      "Epoch [11/50] completed. Total Loss: 181.79093766212463\n",
      "Epoch [12/50] completed. Total Loss: 180.13189482688904\n",
      "Epoch [13/50] completed. Total Loss: 178.5540850162506\n",
      "Epoch [14/50] completed. Total Loss: 177.0247905254364\n",
      "Epoch [15/50] completed. Total Loss: 175.46701097488403\n",
      "Epoch [16/50] completed. Total Loss: 174.062358379364\n",
      "Epoch [17/50] completed. Total Loss: 172.68023681640625\n",
      "Epoch [18/50] completed. Total Loss: 171.1564450263977\n",
      "Epoch [19/50] completed. Total Loss: 169.71874904632568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     24\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Reshape logits and targets for loss computation\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        target_ids = target_ids.view(-1)\n",
    "        \n",
    "        loss = criterion(logits, target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            avg_loss = total_loss / 100\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {avg_loss:.4f}\")\n",
    "            total_loss = 0\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Total Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, and\n",
      "the not banished of future destiny, I should\n",
      "The father.\n",
      "\n",
      "horror,\n",
      "\n",
      "\n",
      "No, or if to the Arabian were\n",
      "the woe, and sometimes you, and\n",
      "\n",
      "”\n",
      "com scene\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=50, temperature=0.8, top_k=50)\n",
    "\n",
    "# Decode the generated ids\n",
    "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27873280"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
